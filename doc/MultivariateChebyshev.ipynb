{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Chebyshev Interpolation\n",
    "\n",
    "Multivariate Chebyshev interpolation is a classical interpolation\n",
    "method. It is applied for various applications in mathematical finance\n",
    "as discussed e.g. in (Gass et al. 2016) and (Poetz 2020). An important\n",
    "implementation of Chebyshev polynomials is the *chebfun* MATLAB project,\n",
    "see (Driscoll, Hale, and Trefethen 2014).\n",
    "\n",
    "In its classical tensor-based form multivariate Chebyshev interpolation\n",
    "suffers from the *curse of dimensionality*. This means that the\n",
    "computational effort grows exponentially with the number of input\n",
    "dimensions of the target function. There are various approaches to\n",
    "circumvent the exponential growth in computational effort. As an\n",
    "example, we mention tensor trains which are recently proposed for\n",
    "function approximation in (Antonov and Piterbarg 2021).\n",
    "\n",
    "With this work we do not aim at lifting the curse of dimensionality.\n",
    "Instead, we want to show how general-purpose high-level linear algebra\n",
    "operations can be used to implement multivariate Chebyshev interpolation\n",
    "efficiently given its intrinsic constraints. The high-level linear\n",
    "algebra operations itself are typically implemented as efficiently as\n",
    "possible by delegating calculations to BLAS routines (Blackford et al.\n",
    "2002) and by applying parallelisation.\n",
    "\n",
    "Chebyshev interpolation is specified on the $D$-dimensional cube\n",
    "$\\left[-1,1\\right]^{D}$. We denote $p=\\left(p_{1},\\ldots,p_{D}\\right)$\n",
    "the elements of that standardised domain $\\left[-1,1\\right]^{D}$.\n",
    "Original model inputs $x$ are assumed to be defined on a general\n",
    "hyper-rectangular domain. Such general domain is transformed into the\n",
    "standardised domain via an element-wise affine transformation.\n",
    "\n",
    "We use the notation $p\\left(x\\right)$ and $x\\left(p\\right)$ to describe\n",
    "the affine transformation from the general input domain to the\n",
    "standardised domain and vice versa.\n",
    "\n",
    "For a scalar parameter $p_{d}\\in\\left[-1,1\\right]$ the Chebyshev\n",
    "polynomial of degree $j$ is denoted $T_{j}\\left(p_{d}\\right)$. The\n",
    "Chebyshev polynomial is defined via\n",
    "$$T_{j}\\left(p_{d}\\right)=\\cos\\left(j\\arccos\\left(p_{d}\\right)\\right).\\label{eq:Chebyshev-polynomial}$$\n",
    "An equivalent representation is given via the recursion\n",
    "$$\\begin{aligned}T_{0}\\left(p_{d}\\right) & =1,\\\\\n",
    "T_{1}\\left(p_{d}\\right) & =p_{d},\\\\\n",
    "T_{j}\\left(p_{d}\\right) & =2p_{d}T_{j-1}\\left(p_{d}\\right)-T_{j-2}\\left(p_{d}\\right).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Multivariate Chebyshev polynomials are defined as products of\n",
    "one-dimensional Chebyshev polynomials. Let\n",
    "$\\bar{j}=\\left(j_{1},\\ldots,j_{D}\\right)$ be a multi-index and\n",
    "$p\\in\\left[-1,1\\right]^{D}$. The multivariate Chebyshev polynomial of\n",
    "degree $\\bar{j}$ is\n",
    "$$T_{\\bar{j}}\\left(p\\right)=\\prod_{d=1}^{D}T_{j_{d}}\\left(p_{d}\\right).$$\n",
    "\n",
    "Tensor based Chebyshev interpolation of a target function $y(x)$ for\n",
    "(multi-index) degree $\\bar{N}=\\left(N_{1},\\ldots,N_{D}\\right)$ is given\n",
    "by\n",
    "$$f(x)=\\sum_{0\\leq\\bar{j}\\leq\\bar{N}}c_{\\bar{j}}T_{\\bar{j}}\\left(p\\left(x\\right)\\right)=\\sum_{j_{1}=0}^{N_{1}}\\cdots\\sum_{j_{D}=0}^{N_{D}}c_{\\left(j_{1},\\ldots,j_{D}\\right)}\\prod_{d=1}^{D}T_{j_{d}}\\left(p_{d}\\right).\\label{eq:Chebyshev-interpolation}$$\n",
    "Here, $p_{d}$ is the $d$-th element of $p\\left(x\\right)$.\n",
    "\n",
    "In order to calculate the coefficients\n",
    "$c_{\\bar{j}}=c_{\\left(j_{1},\\ldots,j_{D}\\right)}$ we introduce the\n",
    "multivariate Chebyshev points (of second kind). We consider a\n",
    "multi-index $\\bar{k}$ and set\n",
    "$$q_{\\bar{k}}=\\left(q_{k_{1}}^{1},\\ldots,q_{k_{D}}^{D}\\right)\\in\\left[-1,1\\right]^{D}$$\n",
    "with\n",
    "$$q_{k_{d}}^{d}=\\cos\\left(\\pi\\frac{k_{d}}{N_{d}}\\right),\\;0\\leq k_{d}\\leq N_{d},\\;d=1,\\ldots,D.\\label{eq:Chebyshev-points}$$\n",
    "The mapping\n",
    "$$x_{\\bar{k}}=x\\left(q_{\\bar{k}}\\right)=x\\left(q_{k_{1}}^{1},\\ldots,q_{k_{D}}^{D}\\right)=\\left[x\\left(q_{k_{1}}^{1}\\right),\\ldots,x\\left(q_{k_{D}}^{D}\\right)\\right]$$\n",
    "defines the affine mapping from the standardised domain\n",
    "$\\left[-1,1\\right]^{D}$ to the domain of the target function.\n",
    "\n",
    "The coefficients $c_{\\bar{j}}=c_{\\left(j_{1},\\ldots,j_{D}\\right)}$ are\n",
    "given as\n",
    "$$c_{\\bar{j}}=\\left(\\prod_{d=1}^{D}\\frac{2^{\\mathbbm{1}_{0<j_{d}<N_{d}}}}{N_{d}}\\right)\\sum_{k_{1}=0}^{N_{1}}\\text{\\textasciiacute\\textasciiacute}\\cdots\\sum_{k_{D}=0}^{N_{D}}\\text{\\textasciiacute\\textasciiacute}y\\left(x\\left(q_{k_{1}}^{1},\\ldots,q_{k_{D}}^{D}\\right)\\right)\\prod_{d=1}^{D}T_{j_{d}}\\left(q_{k_{d}}^{d}\\right).\\label{eq:Chebyshev-coefficients}$$\n",
    "Here, the notation $\\sum\\text{\\textasciiacute\\textasciiacute}$\n",
    "represents the weighted sum where the first and last element are\n",
    "assigned weight $\\frac{1}{2}$ and all other elements are assigned unit\n",
    "weight.\n",
    "\n",
    "A first critical aspect of multivariate Chebyshev interpolation is that\n",
    "the method initially requires $\\prod_{d=1}^{D}\\left(N_{d}+1\\right)$\n",
    "evaluations $y\\left(x\\left(q_{\\bar{k}}\\right)\\right)$ of the target\n",
    "function at the Chebyshev points $q_{\\bar{k}}$. For larger dimensions\n",
    "(e.g. $D>3$) and computationally expensive target functions this can be\n",
    "a limitation.\n",
    "\n",
    "Another critical aspect of multivariate Chebyshev interpolation concerns\n",
    "the linear algebra operations. The evaluation of an interpolation $f(x)$\n",
    "as well as the calibration of each coefficient $c_{\\bar{j}}$ require a\n",
    "calculation of the form\n",
    "$$\\sum_{j_{1}=0}^{N_{1}}\\cdots\\sum_{j_{D}=0}^{N_{D}}a_{\\left(j_{1},\\ldots,j_{D}\\right)}\\prod_{d=1}^{D}b_{j_{d}}.$$\n",
    "A straight forward implementation of such a nested sum involves an\n",
    "iterator along the cartesian product of the indices\n",
    "$$(0,\\ldots,N_{1}),(0,\\ldots,N_{2}),\\ldots,(0,\\ldots,N_{D}).$$ Within\n",
    "each iteration we have $D$ multiplications. This amounts to\n",
    "$D\\,\\prod_{d=1}^{D}\\left(N_{d}+1\\right)$ multiplications potentially\n",
    "followed by an additions. This illustrates the exponential growth of\n",
    "computational effort in terms of number of dimensions $D$.\n",
    "\n",
    "In the following sections we will discuss how to implement above nested\n",
    "sum efficiently by exploiting standardised high-level linear algebra\n",
    "operations available in modern programming environments.\n",
    "\n",
    "# High-level Linear Algebra Operations\n",
    "\n",
    "In this section we discuss linear algebra operations that turn out to be\n",
    "useful for the implementation of multivariate Chebyshev interpolation.\n",
    "Such operations are often available in linear algebra modules of\n",
    "high-level programming languages. Our example implementations are based\n",
    "on Numpy, TensorFlow and Julia. But we aim at avoiding language or\n",
    "module specific implementation choices.\n",
    "\n",
    "#### Multi-dimensional arrays.\n",
    "\n",
    "A guiding principle of our algorithm is the representation of data\n",
    "structures as multi-dimensional arrays. Such multi-dimensional arrays\n",
    "are also called tensors. Tensor operations are further discussed,\n",
    "e.g. in (Golub and Loan 2013), Sec. 12.4.\n",
    "\n",
    "A $D$-dimensional array ${\\cal A}=\\left(a_{\\bar{j}}\\right)$ is a\n",
    "structure consisting of elements $a_{\\bar{j}}\\in\\mathbb{R}$ where\n",
    "$\\bar{j}$ is a multi-index $\\bar{j}=\\left(j_{1},\\ldots,j_{D}\\right)$.\n",
    "For each axis (or *mode*) $d=1,\\ldots,D$ we have an index range\n",
    "$j_{d}=1,\\ldots,N_{d}$. The number of dimensions $D$ is also called the\n",
    "order of the tensor.\n",
    "\n",
    "Obviously, vectors and matrices represent the special cases of one- and\n",
    "two-dimensional arrays or order-1 and order-2 tensors. Scalars can be\n",
    "viewed as order-0 tensors.\n",
    "\n",
    "The tuple $\\left(N_{1},\\ldots,N_{D}\\right)$ represents the shape of the\n",
    "tensor. The shape specifies the index ranges for each axis.\n",
    "\n",
    "Elements of a tensor ${\\cal A}$ are accessed via the function call\n",
    "operator ${\\cal A}\\left(\\cdot\\right)$. That is\n",
    "$${\\cal A}\\left(j_{1},\\ldots,j_{D}\\right)=a_{\\left(j_{1},\\ldots,j_{D}\\right)}.$$\n",
    "Sub-tensors or slices are specified by replacing specific indices\n",
    "$j_{d}$ by “$:$” For example, ${\\cal A}\\left(:,j_{D-1},j_{D}\\right)$ is\n",
    "an order-$D-2$ tensor of shape $\\left(N_{1},\\ldots,N_{D-2}\\right)$.\n",
    "\n",
    "#### Cartesian product of vectors.\n",
    "\n",
    "For a list of vectors $v^{1},\\ldots,v^{D}$ with\n",
    "$v^{d}=\\left(v_{j_{d}}^{d}\\right)_{j_{d}=1}^{N_{d}}$ ($d=1,\\ldots,D$) we\n",
    "define the cartesian product\n",
    "$$V=\\text{product}\\left(v^{1},\\ldots,v^{D}\\right)$$ as the\n",
    "$\\left(\\prod_{d=1}^{D}N_{d}\\right)\\times D$-matrix $V$ with elements\n",
    "$$V=\\left[\\begin{array}{ccccc}\n",
    "v_{1}^{1} & v_{1}^{2} & \\ldots & v_{1}^{D-1} & v_{1}^{D}\\\\\n",
    "v_{1}^{1} & v_{1}^{2} & \\ldots & v_{1}^{D-1} & v_{2}^{D}\\\\\n",
    " &  & \\vdots\\\\\n",
    "v_{N_{1}}^{1} & v_{N_{2}}^{2} & \\ldots & v_{N_{D-1}}^{D-1} & v_{N_{D}-1}^{D}\\\\\n",
    "v_{N_{1}}^{1} & v_{N_{2}}^{2} & \\ldots & v_{N_{D-1}}^{D-1} & v_{N_{D}}^{D}\n",
    "\\end{array}\\right].$$ In this ordering the elements in the last column\n",
    "change fastest and the elements in the first column change slowest.\n",
    "\n",
    "We will apply the cartesian product operation for real vectors as well\n",
    "as for index vectors. In particular, the cartesian product of indices\n",
    "$$J=\\text{product}\\left(\\left(1,\\ldots,N_{1}\\right),\\ldots,\\left(1,\\ldots,N_{D}\\right)\\right)$$\n",
    "yields a vector of multi-indices $J=\\left(\\bar{j}\\right)_{\\bar{j}}$ that\n",
    "allows to iterate the elements of a tensor\n",
    "${\\cal A}=\\left(a_{\\bar{j}}\\right)$.\n",
    "\n",
    "#### Re-shaping tensors.\n",
    "\n",
    "Reshaping changes the order of a tensor but keeps the data elements\n",
    "unchanged. The most basic form of re-shaping a tensor is the flattening\n",
    "or vectorisation. We define\n",
    "$$\\text{vec}\\left({\\cal A}\\right)=\\left[\\begin{array}{c}\n",
    "a_{\\left(1,1,\\ldots,1,1\\right)}\\\\\n",
    "a_{\\left(1,1,\\ldots,1,2\\right)}\\\\\n",
    "\\vdots\\\\\n",
    "a_{\\left(N_{1},N_{2},\\ldots,N_{D-1},N_{D}-1\\right)}\\\\\n",
    "a_{\\left(N_{1},N_{2},\\ldots,N_{D-1},N_{D}\\right)}\n",
    "\\end{array}\\right]=\\left[a_{\\bar{j}}\\right]_{\\bar{j}\\in J}.$$ That is,\n",
    "we align the tensor elements with the last axis changing fastest and the\n",
    "first axis changing slowest similarly as in the cartesian product\n",
    "specification.\n",
    "\n",
    "A general re-shape operation\n",
    "$${\\cal B}=\\text{reshape}\\left({\\cal A},\\left(M_{1},\\ldots,M_{E}\\right)\\right)$$\n",
    "of a tensor ${\\cal A}$ with shape $\\left(N_{1},\\ldots,N_{D}\\right)$ into\n",
    "a tensor ${\\cal B}$ with shape $\\left(M_{1},\\ldots,M_{E}\\right)$ and\n",
    "$$\\prod_{d=1}^{D}N_{d}=\\prod_{e=1}^{E}M_{e}$$ is defined via\n",
    "$$\\text{vec}\\left({\\cal A}\\right)=\\text{vec}\\left({\\cal B}\\right).$$\n",
    "\n",
    "#### Element-wise tensor multiplication with broadcasting.\n",
    "\n",
    "Element-wise tensor multiplication is used to delegate calculations to\n",
    "efficient low-level implementations utilising e.g. BLAS routines and\n",
    "parallelisation. This approach is particularly efficient when combined\n",
    "with the concept of broadcasting.\n",
    "\n",
    "Consider two tensors ${\\cal A}=\\left(a_{\\bar{j}}\\right)$ and\n",
    "${\\cal B}=\\left(b_{\\bar{j}}\\right)$ with shape\n",
    "$\\left(N_{1},\\ldots,N_{D}\\right)$ and $\\left(M_{1},\\ldots,M_{D}\\right)$.\n",
    "We impose the constraint that\n",
    "$$N_{d}=M_{d}\\;\\text{or}\\;N_{d}=1\\;\\text{or}\\;M_{d}=1\\;\\text{for}\\;d=1,\\ldots,D.$$\n",
    "The element-wise product with broadcasting\n",
    "$${\\cal C}={\\cal A}\\;{.*}\\;{\\cal B}$$ yields a tensor ${\\cal C}$ with\n",
    "shape\n",
    "$$\\left(\\max\\left\\{ N_{1},M_{1}\\right\\} ,\\ldots,\\max\\left\\{ N_{D},M_{D}\\right\\} \\right).$$\n",
    "The elements $c_{\\bar{j}}=c_{\\left(j_{1},\\ldots,j_{D}\\right)}$ of the\n",
    "resulting tensor ${\\cal C}$ are\n",
    "$$c_{\\left(j_{1},\\ldots,j_{D}\\right)}=a_{\\left(\\min\\left\\{ j_{1},N_{1}\\right\\} ,\\ldots,\\min\\left\\{ j_{D},N_{D}\\right\\} \\right)}\\cdot b_{\\left(\\min\\left\\{ j_{1},M_{1}\\right\\} ,\\ldots,\\min\\left\\{ j_{D},M_{D}\\right\\} \\right)}.$$\n",
    "\n",
    "Element-wise multiplication with broadcasting is the standard behaviour\n",
    "for multiplication of multi-dimensional arrays in Numpy and TensorFlow.\n",
    "In Julia it is implemented by the .\\* operator.\n",
    "\n",
    "#### Generalised matrix multiplication.\n",
    "\n",
    "The Python Enhancement Proposal (PEP) 465 (Smith 2014) specifies a\n",
    "matrix multiplication that generalises to multi-dimensional arrays. This\n",
    "operation is implemented in Numpy and TensorFlow as the *matmul*\n",
    "function.\n",
    "\n",
    "Suppose, we have two tensors ${\\cal A}$ and ${\\cal B}$ with shape\n",
    "$\\left(N_{1},\\ldots,N_{D}\\right)$ and $\\left(M_{1},\\ldots,M_{D}\\right)$.\n",
    "We require that $D\\geq2$,\n",
    "$$N_{d}=M_{d}\\;\\text{or}\\;N_{d}=1\\;\\text{or}\\;M_{d}=1\\;\\text{for}\\;d=1,\\ldots,D-2,$$\n",
    "and $$N_{D}=M_{D-1}.$$ The generalised matrix multiplication is defined\n",
    "as $${\\cal C}=\\text{matmul}\\left({\\cal A},{\\cal B}\\right).$$ The result\n",
    "tensor ${\\cal C}$ is of shape\n",
    "$$\\left(\\max\\left\\{ N_{1},M_{1}\\right\\} ,\\ldots,\\max\\left\\{ N_{D-2},M_{D-2}\\right\\} ,N_{D-1},M_{D}\\right).$$\n",
    "And the elements of ${\\cal C}$ are calculated as\n",
    "$${\\cal C}\\left(:,i,j\\right)=\\sum_{k=1}^{N_{D}}{\\cal A}\\left(:,i,k\\right)\\;{.*}\\;{\\cal B}\\left(:,k,j\\right)$$\n",
    "for $i=1,\\ldots,N_{d-1}$ and $j=1,\\ldots,M_{d}$. Here,\n",
    "${\\cal C}\\left(:,i,j\\right)$ is the tensor of order $D-2$ where we fix\n",
    "the last two axes of ${\\cal C}$. Analogously,\n",
    "${\\cal A}\\left(:,i,k\\right)$ and ${\\cal B}\\left(:,k,j\\right)$ are\n",
    "specified.\n",
    "\n",
    "We note that the generalised matrix multiplication can be related to the\n",
    "*modal product* of tensors and matrices. Consider a matrix ${\\cal M}$ of\n",
    "shape $\\left(M_{1},M_{2}\\right)$ with $M_{2}=N_{d}$. The mode-$d$\n",
    "product $${\\cal C}={\\cal A}\\times_{d}{\\cal M}$$ yields a tensor of shape\n",
    "$$\\left(N_{1},\\ldots,N_{d-1},M_{1},N_{d+1},N_{D}\\right).$$ The elements\n",
    "of ${\\cal C}=\\left(c_{\\left(j_{1},\\ldots,j_{D}\\right)}\\right)$ are\n",
    "calculated as\n",
    "$${\\cal C}{\\left(j_{1},\\ldots,j_{d-1},i,j_{d+1},j_{D}\\right)}=\\sum_{k=1}^{N_{d}}{\\cal M}\\left(i,k\\right){\\cal A}{\\left(j_{1},\\ldots,j_{d-1},k,j_{d+1},j_{D}\\right)}$$\n",
    "for $i=1,\\ldots,M_{1}$.\n",
    "\n",
    "It turns out that the mode-$D$ product along the last axis is\n",
    "$${\\cal A}\\times_{D}{\\cal M}=\\text{matmul}\\left({\\cal A},\\text{reshape}\\left({\\cal M}^{\\top},\\left(1,\\ldots,1,M_{2},M_{1}\\right)\\right)\\right).$$\n",
    "We will use this observation to formulate Chebyshev interpolation as a\n",
    "sequence of high-level $\\text{matmul}\\left(\\cdot\\right)$ operations\n",
    "where we can rely on efficient low-level implementations.\n",
    "\n",
    "# Reformulated Chebyshev Interpolation\n",
    "\n",
    "We return to the task of calculating nested sums of the form\n",
    "$$\\sum_{j_{1}=0}^{N_{1}}\\cdots\\sum_{j_{D}=0}^{N_{D}}a_{\\left(j_{1},\\ldots,j_{D}\\right)}\\prod_{d=1}^{D}b_{j_{d}}.$$\n",
    "The coefficients $a_{\\left(j_{1},\\ldots,j_{D}\\right)}$ can be aligned in\n",
    "an order-$D$ tensor ${\\cal A}$. of shape\n",
    "$\\left(N_{1}+1,\\ldots,N_{D}+1\\right)$. Similarly, the Chebyshev\n",
    "polynomial values $b_{j_{d}}$ can be arranged as $D$ matrices\n",
    "${\\cal B}^{d}$ of shape $\\left(1,N_{d}+1\\right)$.\n",
    "\n",
    "With this notation the nested sum becomes a sequence of modal products\n",
    "$$\\sum_{j_{1}=0}^{N_{1}}\\cdots\\sum_{j_{D}=0}^{N_{D}}a_{\\left(j_{1},\\ldots,j_{D}\\right)}\\prod_{d=1}^{D}b_{j_{d}}={\\cal C}\\left(1,\\ldots,1\\right)$$\n",
    "where the order $D$ tensor ${\\cal C}$ with shape\n",
    "$\\left(1,\\ldots,1\\right)$ is\n",
    "$${\\cal C}=\\left(\\left({\\cal A}\\times_{D}{\\cal B}^{D}\\right)\\times_{D-1}\\ldots\\right)\\times_{1}{\\cal B}^{1}.$$\n",
    "\n",
    "The property that the multivariate Chebyshev interpolation formula can\n",
    "be written as modal product is also observed in (Poetz 2020, sec. 5.1).\n",
    "We also note that the sequence of modal products is invariant with\n",
    "respect to its ordering. See (Golub and Loan 2013Theorem 12.4.1). Thus,\n",
    "we could also calculate\n",
    "$${\\cal C}=\\left(\\left({\\cal A}\\times_{1}{\\cal B}^{1}\\right)\\times_{2}\\ldots\\right)\\times_{D}{\\cal B}^{D}.$$\n",
    "\n",
    "#### Chebyshev batch calculation.\n",
    "\n",
    "The interpolation function $f$ from equation\n",
    "(<a href=\"#eq:Chebyshev-interpolation\" data-reference-type=\"ref\" data-reference=\"eq:Chebyshev-interpolation\">[eq:Chebyshev-interpolation]</a>)\n",
    "often needs to be evaluated for various inputs $x^{1},\\ldots,x^{N}$. In\n",
    "such a context we call $N$ (without subscript) the batch size for\n",
    "evaluation. In order to utilize BLAS routines and parallelisation we\n",
    "want to avoid manual iteration over the elements of a batch. Instead, we\n",
    "carefully use broadcasting to vectorise calculations.\n",
    "\n",
    "Input to the Chebyshev batch calculation are a matrix ${\\cal P}$ and an\n",
    "order-$D$ tensor ${\\cal C}$. The matrix ${\\cal P}$ is of shape\n",
    "$\\left(D,N\\right)$ and consists of points from the standardised domain\n",
    "$\\left[-1,1\\right]^{D}$. That is,\n",
    "$${\\cal P}=\\left[p\\left(x^{1}\\right),\\ldots,p\\left(x^{N}\\right)\\right].$$\n",
    "The tensor ${\\cal C}$ is of shape $\\left(N_{1}+1,\\ldots,N_{D}+1\\right)$\n",
    "and for the usage of interpolation consists of the Chebyshev\n",
    "coefficients $c_{\\bar{j}}$ from rquation\n",
    "(<a href=\"#eq:Chebyshev-coefficients\" data-reference-type=\"ref\" data-reference=\"eq:Chebyshev-coefficients\">[eq:Chebyshev-coefficients]</a>).\n",
    "\n",
    "For each axis $d$ and input row ${\\cal P}\\left(d,:\\right)$ we calculate\n",
    "a matrix of Chebyshev polynomial values ${\\cal T}^{d}$ of shape\n",
    "$\\left(N_{d}+1,N\\right)$ with entries\n",
    "$$\\begin{aligned}{\\cal T}^{d}\\left(1,:\\right) & =\\left(1,\\ldots,1\\right),\\\\\n",
    "{\\cal T}^{d}\\left(2,:\\right) & ={\\cal P}\\left(d,:\\right),\\\\\n",
    "{\\cal T}^{d}\\left(j,:\\right) & =2\\,{\\cal P}\\left(d,:\\right)\\;{.*}\\;{\\cal T}^{d}\\left(j-1,:\\right)-{\\cal T}^{d}\\left(j-2,:\\right),\n",
    "\\end{aligned}\n",
    "\\label{eq:multi-Chebyshev-polynomial}$$ for $j=3,\\ldots,N_{d}+1$.\n",
    "\n",
    "With algorithm\n",
    "<a href=\"#alg:Chebyshev-batch-calculation\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-batch-calculation\">[alg:Chebyshev-batch-calculation]</a>\n",
    "we calculate a vector\n",
    "$${\\cal R}=\\left[f\\left(x^{1}\\right),\\ldots,f\\left(x^{N}\\right)\\right]^{\\top}.$$\n",
    "\n",
    "Initialise\n",
    "${\\cal R}\\leftarrow\\text{reshape}\\left({\\cal C},\\left(1,N_{1}+1,\\ldots,N_{D}+1\\right)\\right)$\n",
    "by adding a trivial first axis. Remove remaining trivial axis\n",
    "${\\cal R}\\leftarrow\\text{reshape}\\left({\\cal R},\\left(N\\right)\\right)$.\n",
    "\n",
    "We assess the computational effort of the Chebyshev batch calculation in\n",
    "algorithm\n",
    "<a href=\"#alg:Chebyshev-batch-calculation\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-batch-calculation\">[alg:Chebyshev-batch-calculation]</a>.\n",
    "Re-shape and matrix transposition operations are cheap because they do\n",
    "not require data access or data manipulation. We count multiplications\n",
    "which may be followed by an addition as a single operation.\n",
    "\n",
    "Chebyshev matrix calculation in step 3 amounts to\n",
    "$$2N\\sum_{d=1}^{D}\\left(N_{d}-1\\right)$$ operations. This is more or\n",
    "less ${\\cal O}\\left(N\\right)$ and relatively cheap.\n",
    "\n",
    "The computationally expensive step is the generalised matrix\n",
    "multiplication in step 9. Here we count\n",
    "$$N\\sum_{d-1}^{D}\\underbrace{\\left(\\prod_{j=1}^{d-1}\\left(N_{j}+1\\right)\\right)}_{{\\text{broadcasting}}}\\underbrace{\\left(N_{d}+1\\right)}_{\\text{multiplications\\;.*}}=N\\sum_{d-1}^{D}\\,\\prod_{j=1}^{d}\\left(N_{j}+1\\right)$$\n",
    "operations. For $\\hat{N}=\\max\\left\\{ N_{1},\\ldots,N_{D}\\right\\} \\geq1$\n",
    "we get the estimate\n",
    "$$N\\sum_{d-1}^{D}\\,\\prod_{j=1}^{d}\\left(N_{j}+1\\right)\\leq N\\sum_{d-1}^{D}\\left(\\hat{N}+1\\right)^{d}<2N\\left(\\hat{N}+1\\right)^{D}.$$\n",
    "The proposed algorithm still suffers from the exponential growth in the\n",
    "number of dimensions $D$. However, we save a factor of $D/2$ compared to\n",
    "a standard implementation via cartesian product.\n",
    "\n",
    "#### Chebyshev coefficient calibration.\n",
    "\n",
    "Now, we analyse the calculation of the Chebyshev coefficients\n",
    "$c_{\\bar{j}}$ from equation\n",
    "(<a href=\"#eq:Chebyshev-coefficients\" data-reference-type=\"ref\" data-reference=\"eq:Chebyshev-coefficients\">[eq:Chebyshev-coefficients]</a>),\n",
    "$$c_{\\bar{j}}=\\underbrace{\\left(\\prod_{d=1}^{D}\\frac{2^{\\mathbbm{1}_{0<j_{d}<N_{d}}}}{N_{d}}\\right)}_{v_{\\bar{j}}}\\sum_{k_{1}=0}^{N_{1}}\\text{\\textasciiacute\\textasciiacute}\\cdots\\sum_{k_{D}=0}^{N_{D}}\\text{\\textasciiacute\\textasciiacute}y\\left(x\\left(q_{k_{1}}^{1},\\ldots,q_{k_{D}}^{D}\\right)\\right)\\prod_{d=1}^{D}T_{j_{d}}\\left(q_{k_{d}}^{d}\\right)$$\n",
    "for\n",
    "$$\\bar{j}\\in J=\\text{product}\\left(\\left(0,\\ldots,N_{1}\\right),\\ldots,\\left(0,\\ldots,N_{D}\\right)\\right).$$\n",
    "We will demonstrate how this calculation can be related to the Chebyshev\n",
    "batch calculation from algorithm\n",
    "<a href=\"#alg:Chebyshev-batch-calculation\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-batch-calculation\">[alg:Chebyshev-batch-calculation]</a>.\n",
    "\n",
    "Starting point for the calculation are multivariate Chebyshev points\n",
    "$q_{\\bar{k}}=\\left(q_{k_{1}}^{1},\\ldots,q_{k_{D}}^{D}\\right)\\in\\left[-1,1\\right]^{D}$\n",
    "from equation\n",
    "(<a href=\"#eq:Chebyshev-points\" data-reference-type=\"ref\" data-reference=\"eq:Chebyshev-points\">[eq:Chebyshev-points]</a>)\n",
    "for $\\bar{k}\\in J$. The multivariate Chebyshev points are collected as\n",
    "cartesian product in the matrix ${\\cal Q}$of shape\n",
    "$\\left(\\prod_{d=1}^{D}\\left(N_{d}+1\\right),D\\right)$ such that\n",
    "$$Q=\\left[q_{\\bar{k}}\\right]_{\\bar{k}\\in J}.$$ For each row\n",
    "$q_{\\bar{k}}$ in $Q$ we calculate the target function value vector\n",
    "$\\left[y_{\\bar{k}}\\right]_{\\bar{k}\\in J}$ such that\n",
    "$$\\left[y_{\\bar{k}}\\right]_{\\bar{k}\\in J}=\\left[y\\left(x\\left(q_{\\bar{k}}\\right)\\right)\\right]_{\\bar{k}\\in J}.\\label{eq:function-values}$$\n",
    "The weighs of the $\\sum\\text{\\textasciiacute\\textasciiacute}$ operator\n",
    "are\n",
    "$$\\left[w_{\\bar{k}}\\right]_{\\bar{k}\\in J}=\\left[2^{-\\left(N_{d}-\\sum_{d=1}^{D}\\mathbbm{1}_{0<k_{d}<N_{d}}\\right)}\\right]_{\\bar{k}\\in J}.\\label{eq:function-weights}$$\n",
    "Similarly, we calcute the vector of coefficient weights\n",
    "$$\\left[v_{\\bar{j}}\\right]_{\\bar{j}\\in J}=\\left[\\prod_{d=1}^{D}\\frac{2^{\\mathbbm{1}_{0<j_{d}<N_{d}}}}{N_{d}}\\right]_{\\bar{j}\\in J}.\\label{eq:coefficient-weights}$$\n",
    "We note that calculation of the weight vectors\n",
    "$\\left[w_{\\bar{k}}\\right]_{\\bar{k}\\in J}$ and\n",
    "$\\left[v_{\\bar{j}}\\right]_{\\bar{j}\\in J}$ typically can also be\n",
    "vectorisied and do not require manual iteration over the index $\\bar{k}$\n",
    "or $\\bar{j}$.\n",
    "\n",
    "With the weights $\\left[w_{\\bar{k}}\\right]_{\\bar{k}\\in J}$ and function\n",
    "values $\\left[y_{\\bar{k}}\\right]_{\\bar{k}\\in J}$ we can re-write the\n",
    "coefficient calculation using standard sum operator and\n",
    "$$\\frac{c_{\\bar{j}}}{v_{\\bar{j}}}=\\sum_{k_{1}=0}^{N_{1}}\\cdots\\sum_{k_{D}=0}^{N_{D}}w_{\\bar{k}}y_{\\bar{k}}\\prod_{d=1}^{D}T_{j_{d}}\\left(q_{k_{d}}^{d}\\right).$$\n",
    "From equations\n",
    "(<a href=\"#eq:Chebyshev-polynomial\" data-reference-type=\"ref\" data-reference=\"eq:Chebyshev-polynomial\">[eq:Chebyshev-polynomial]</a>)\n",
    "and\n",
    "(<a href=\"#eq:multi-Chebyshev-polynomial\" data-reference-type=\"ref\" data-reference=\"eq:multi-Chebyshev-polynomial\">[eq:multi-Chebyshev-polynomial]</a>)\n",
    "we observe that $$\\begin{aligned}\n",
    "T_{j_{d}}\\left(q_{k_{d}}^{d}\\right) & =\\cos\\left(j_{d}\\arccos\\left(\\cos\\left(\\pi\\frac{k_{d}}{N_{d}}\\right)\\right)\\right)\\\\\n",
    " & =\\cos\\left(k_{d}\\arccos\\left(\\cos\\left(\\pi\\frac{j_{d}}{N_{d}}\\right)\\right)\\right)\\\\\n",
    " & =T_{k_{d}}\\left(q_{j_{d}}^{d}\\right).\\end{aligned}$$ This yields the\n",
    "desired form\n",
    "$$\\frac{c_{\\bar{j}}}{v_{\\bar{j}}}=\\sum_{k_{1}=0}^{N_{1}}\\cdots\\sum_{k_{D}=0}^{N_{D}}w_{\\bar{k}}y_{\\bar{k}}\\prod_{d=1}^{D}T_{k_{d}}\\left(q_{j_{d}}^{d}\\right).\\label{eq:Chebyshev-calibration}$$\n",
    "The right-hand side of equation\n",
    "(<a href=\"#eq:Chebyshev-calibration\" data-reference-type=\"ref\" data-reference=\"eq:Chebyshev-calibration\">[eq:Chebyshev-calibration]</a>)\n",
    "is of the same form as the Chebyshev interpolation formula in equation\n",
    "(<a href=\"#eq:Chebyshev-interpolation\" data-reference-type=\"ref\" data-reference=\"eq:Chebyshev-interpolation\">[eq:Chebyshev-interpolation]</a>).\n",
    "We only need to identify the coefficients $c_{\\bar{k}}$ with the\n",
    "weighted function values $w_{\\bar{k}}y_{\\bar{k}}$ and evaluate the\n",
    "interpolation at the Chebyshev points $q_{\\bar{j}}$. Thus, we can re-use\n",
    "algorithm\n",
    "<a href=\"#alg:Chebyshev-batch-calculation\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-batch-calculation\">[alg:Chebyshev-batch-calculation]</a>\n",
    "for efficient implementation.\n",
    "\n",
    "We summarise the Chebyshev coefficient calibration in algorithm\n",
    "<a href=\"#alg:Chebyshev-coefficient-calibration\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-coefficient-calibration\">[alg:Chebyshev-coefficient-calibration]</a>.\n",
    "\n",
    "Calculate\n",
    "$J=\\text{product}\\left(\\left(0,\\ldots,N_{1}\\right),\\ldots,\\left(0,\\ldots,N_{D}\\right)\\right)$.\n",
    "Calculate cartesian product of Chebyshev points\n",
    "$Q=\\left[q_{\\bar{k}}\\right]_{\\bar{k}\\in J}$ from\n",
    "(<a href=\"#eq:Chebyshev-points\" data-reference-type=\"ref\" data-reference=\"eq:Chebyshev-points\">[eq:Chebyshev-points]</a>).\n",
    "Calculate function values $\\left[y_{\\bar{k}}\\right]_{\\bar{k}\\in J}$ from\n",
    "(<a href=\"#eq:function-values\" data-reference-type=\"ref\" data-reference=\"eq:function-values\">[eq:function-values]</a>)\n",
    "Calculate function weights $\\left[w_{\\bar{k}}\\right]_{\\bar{k}\\in J}$\n",
    "from\n",
    "(<a href=\"#eq:function-weights\" data-reference-type=\"ref\" data-reference=\"eq:function-weights\">[eq:function-weights]</a>)\n",
    "Form coefficient tensor ${\\cal Y} = \\text{reshape}\\left(\n",
    "    \\left[w_{\\bar{k}}\\right]_{\\bar{k}\\in J}\\;.*\\;\\left[y_{\\bar{k}}\\right]_{\\bar{k}\\in J},\n",
    "    \\left(N_1+1,\\ldots,N_D+1\\right)\n",
    "  \\right)$ Call algorithm\n",
    "<a href=\"#alg:Chebyshev-batch-calculation\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-batch-calculation\">[alg:Chebyshev-batch-calculation]</a>\n",
    "with inputs ${\\cal Y}$ and $Q^\\top$. This yields a vector\n",
    "$\\left[r_{\\bar{j}}\\right]_{\\bar{j}\\in J}$. Calculate coefficient weights\n",
    "$\\left[v_{\\bar{j}}\\right]_{\\bar{j}\\in J}$ from\n",
    "(<a href=\"#eq:coefficient-weights\" data-reference-type=\"ref\" data-reference=\"eq:coefficient-weights\">[eq:coefficient-weights]</a>).\n",
    "Calculate Chebyshev coefficient tensor ${\\cal C} = \\text{reshape}\\left(\n",
    "    \\left[v_{\\bar{j}}\\right]_{\\bar{j}\\in J}\\;.*\\;\\left[r_{\\bar{j}}\\right]_{\\bar{j}\\in J},\n",
    "    \\left(N_1+1,\\ldots,N_D+1\\right)\n",
    "  \\right)$\n",
    "\n",
    "The key drivers of compuatational effort for algorithm\n",
    "<a href=\"#alg:Chebyshev-coefficient-calibration\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-coefficient-calibration\">[alg:Chebyshev-coefficient-calibration]</a>\n",
    "lie in the function evaluation in step 3 and in the call of algorithm\n",
    "<a href=\"#alg:Chebyshev-batch-calculation\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-batch-calculation\">[alg:Chebyshev-batch-calculation]</a>\n",
    "in step 6. Step 3 requires $\\prod_{d=1}^{D}\\left(N_{d}+1\\right)$\n",
    "evaluations of the target function. In step 6 we call algorithm\n",
    "<a href=\"#alg:Chebyshev-batch-calculation\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-batch-calculation\">[alg:Chebyshev-batch-calculation]</a>\n",
    "with batch size $N=\\prod_{d=1}^{D}\\left(N_{d}+1\\right)$. Consequently,\n",
    "we end up with\n",
    "$$\\prod_{d=1}^{D}\\left(N_{d}+1\\right)\\,\\sum_{d-1}^{D}\\,\\prod_{j=1}^{d}\\left(N_{j}+1\\right)\\sim\\prod_{d=1}^{D}\\left(N_{d}+1\\right)^{2}$$\n",
    "operations. This definitely remains the bottleneck of the method for\n",
    "larger dimensions.\n",
    "\n",
    "Besides the intrinsic computational effort,\n",
    "algorithm<a href=\"#alg:Chebyshev-batch-calculation\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-batch-calculation\">[alg:Chebyshev-batch-calculation]</a>\n",
    "and algorithm\n",
    "<a href=\"#alg:Chebyshev-coefficient-calibration\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-coefficient-calibration\">[alg:Chebyshev-coefficient-calibration]</a>\n",
    "demontrate how calculations can be delegated to a few calls of\n",
    "generalised matrix multiplications. This allows exploiting efficient\n",
    "BLAS implementations and parallelisation.\n",
    "\n",
    "#### Implementation.\n",
    "\n",
    "Implementations of\n",
    "algorithm<a href=\"#alg:Chebyshev-batch-calculation\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-batch-calculation\">[alg:Chebyshev-batch-calculation]</a>\n",
    "and algorithm\n",
    "<a href=\"#alg:Chebyshev-coefficient-calibration\" data-reference-type=\"ref\" data-reference=\"alg:Chebyshev-coefficient-calibration\">[alg:Chebyshev-coefficient-calibration]</a>\n",
    "are provided at\n",
    "[github.com/sschlenkrich/MultivariateChebyshev](github.com/sschlenkrich/MultivariateChebyshev).\n",
    "We choose Numpy and TensorFlow as packages for high-level linear algebra\n",
    "operations. Both packages are available in the Python programing\n",
    "language. Moreover, we implement the algorithms in the Julia programing\n",
    "language. Julia is designed as high-level high-performance language. If\n",
    "nedded, Julia can also be accessed from Python. A comparison of the\n",
    "implementations in `src/multivariate_chebyshev_numpy.py`,\n",
    "`src/multivariate_chebyshev_tensorflow.py` and\n",
    "`src/multivariate_chebyshev_julia.jl` demonstrates that the proposed\n",
    "algorithms are rather generic.\n",
    "\n",
    "We use above implementations to demonstrate the usage of multivariate\n",
    "Chebyshev implementation and to assess which module might be best suited\n",
    "from a performance perspective.\n",
    "\n",
    "# Case Study: Implied Volatility Surface of Heston Model\n",
    "\n",
    "-   Heston model and model parameters\n",
    "\n",
    "-   Vanilla option pricing via QuantLib\n",
    "\n",
    "-   smile parametrisation\n",
    "\n",
    "-   Chebyshev parametrisation: parameter ranges, degrees, resulting\n",
    "    number of coefficients\n",
    "\n",
    "-   smile and term structure approximation for various parameter\n",
    "    scenarios\n",
    "\n",
    "-   (quasi) random sample test(s)\n",
    "\n",
    "-   limitation: extrapolation\n",
    "\n",
    "Antonov, A., and V. Piterbarg. 2021. “Alternatives to Deep Neural\n",
    "Networks for Function Approximations in Finance.”\n",
    "*Https://Ssrn.com/Abstract=3958331*.\n",
    "\n",
    "Blackford, L Susan, Antoine Petitet, Roldan Pozo, Karin Remington, R\n",
    "Clint Whaley, James Demmel, Jack Dongarra, et al. 2002. “An Updated Set\n",
    "of Basic Linear Algebra Subprograms (BLAS).” *ACM Transactions on\n",
    "Mathematical Software* 28 (2): 135–51.\n",
    "\n",
    "Driscoll, T. A., N. Hale, and L. N. Trefethen. 2014. *Chebfun Guide*.\n",
    "Pafnuty Publications.\n",
    "\n",
    "Gass, M., K. Glau, M. Mahlstedt, and M Mair. 2016. “Chebyshev\n",
    "Interpolation for Parametric Option Pricing.”\n",
    "*Https://Arxiv.org/Abs/1505.04648*.\n",
    "\n",
    "Golub, G. H., and C. F. Van Loan. 2013. *Matrix Computations*. GHU\n",
    "Press.\n",
    "\n",
    "Poetz, C. 2020. “Function Approximation for Option Pricing and Risk\n",
    "Management - Methods, Theory and Applications.” Ph.D. thesis.\n",
    "\n",
    "Smith, N. J. 2014. “PEP 465 - a Dedicated Infix Operator for Matrix\n",
    "Multiplication.” https://peps.python.org/pep-0465/."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

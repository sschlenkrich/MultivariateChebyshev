{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heston Volatility Approximation\n",
    "\n",
    "In this notebook we implement the Heston implied volatility approximation via multivariate Chebyshev interpolation.\n",
    "\n",
    "The analysis is structured as follows:\n",
    "\n",
    "1. Summarize Heston model and used Parametrisation\n",
    "\n",
    "1. Compare results for the multivariate Chebyshev implementations using Numpy, Tensorflow and Julia.\n",
    "\n",
    "2. Analyse approximation accuracy for Heston implied volatility use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Heston Model\n",
    "\n",
    "We summarise the Heston model to put our example and parametrisation choices into the wider context of implied volatility modelling. Heston model is a model for the price process of a financial asset $S_t$. The price process $S_t$ is described by the diffusion\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  d S_t   &= \\mu S_t dt + \\sqrt{v_t} S_t d W^S_t, \\\\\n",
    "  d v_t &= \\kappa \\left(\\theta - v_t\\right) dt + \\xi \\sqrt{v_t} d W^v_t, \\\\\n",
    "  d W^S_t d W^v_t &= \\rho dt\n",
    "\\end{aligned}\n",
    "$$\n",
    "with initial conditions $S_0>0$ and $v_0>0$ at $t=0$ and correlated Brownian motions $W^S_t$ and $W^v_t$.\n",
    "\n",
    "For option pricing methods in Heston model we refer to the standard literature. Implied volatilities for a given term (or time to maturity) $T$ and strike price $K$ are obtained by inverting Black's formula given a forward Vanilla option price derived in Heston model.\n",
    "\n",
    "For implied volatility modelling we are interested in forward prices. As a consequence, we can disregard the drift $\\mu$. Implied volatility in Heston model is driven by the parameters of the squared volatility process $v_t$. In particular, we have the following properties.\n",
    "\n",
    "  - $\\sqrt{v_0}$ controls short term volatility.\n",
    "  - $\\sqrt{\\theta}$ controls long term volatility.\n",
    "  - $\\log(2) / \\kappa$ represents the half life of the expectation of $v_t$ moving from $v_0$ to $\\theta$.\n",
    "    This controls the term structure of at-the-money volatilities.\n",
    "  - $\\rho$ controls the volatility skew (or volatility slope in strike direction).\n",
    "  - $\\xi$ controls the volatility smile (or volatility curvature in strike direction).\n",
    "\n",
    "For our parametrisation of implied volatilities we will use two properties of Heston model.\n",
    "\n",
    "**Expectation of squared volatility.**\n",
    "\n",
    "The expectation of the squared volatility $v_T$ for a given term $T\\geq 0$ is given as\n",
    "$$\n",
    "  \\mathbb{E} \\left[ v_T \\right] = v_0 e^{-\\kappa T} + \\theta \\left(1 - e^{-\\kappa T}\\right).\n",
    "$$\n",
    "We use this this property to define an *average standard deviation* of asset prices as\n",
    "$$\n",
    "  \\nu_{(v_0, \\theta, \\kappa)}(T) = \\sqrt{\\left[v_0 e^{-\\kappa T} + \\theta \\left(1 - e^{-\\kappa T}\\right)\\right] T}.\n",
    "$$\n",
    "The average standard deviation is used to normalise option strikes $K$. That is, We define the option moneyness as\n",
    "$$\n",
    "  {\\cal M} = \\frac{\\log\\left(K/S_0\\right)}{\\nu_{(v_0, \\theta, \\kappa)}(T)}.\n",
    "$$\n",
    "\n",
    "**Feller condition.**\n",
    "\n",
    "The Feller condition for the squared volatility process is\n",
    "$$\n",
    "  \\xi^2 \\leq 2 \\kappa \\theta.\n",
    "$$\n",
    "This condition ensures that the squared volatility process remains positive, i.e. $v_t > 0$ for $t>0$. Violation of Feller condition e.g. by high vol-of-vol parameter $\\xi$ is typically accepted to achieve reasonable fits in calibrations. However, high vol-of-vol parameters may cause numerical instabilities.\n",
    "\n",
    "In order to control (or limit) the extend of Feller condition violation we use a *Feller factor* parameter to parametrise volatility smile. We define the Feller factor as\n",
    "$$\n",
    "  {\\cal F} = \\frac{\\xi^2}{2 \\kappa \\theta}.\n",
    "$$ \n",
    "\n",
    "**Implied volatility function parametrisation**\n",
    "\n",
    "For a given model or market observation implied volatility $\\sigma_{IV}$ is a function of the option term $T$ and the strike price $K$. That is\n",
    "$$\n",
    "  \\sigma_{IV}\\left(T, K\\right)\n",
    "$$\n",
    "also forms a volatility surface.\n",
    "\n",
    "For this analysis we extend the volatility surface function by the Heston model parameters and apply a parameter transformation. Our target function is $f:{\\cal D}\\rightarrow R$ with ${\\cal D} \\subset \\mathbb{R}^8$ such that\n",
    "$$\n",
    "  f\\left(x\\right) = \\sigma_{IV}\\left(T, K; S_0, v_0, \\theta, \\kappa, \\rho, \\xi \\right).\n",
    "$$\n",
    "The target function argument $x$ is specified as\n",
    "$$\n",
    "  x = \\left[\n",
    "  \\begin{array}{c}\n",
    "    x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ x_7\n",
    "  \\end{array}\n",
    "  \\right]\n",
    "  = \\left[\n",
    "  \\begin{array}{c}\n",
    "    T \\\\ {\\cal M} \\\\ S_0 \\\\ \\sqrt{v_0} \\\\ \\sqrt{\\theta / v_0} \\\\ 0.7 / \\kappa \\\\ \\rho \\\\ {\\cal F}\n",
    "  \\end{array}\n",
    "  \\right].\n",
    "$$\n",
    "The input domain ${\\cal D}$ is the hyper-rectangle with boundaries as specified below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    'term'            : ( 1/12, 5.0  ),\n",
    "    'moneyness'       : ( -3.0, 3.0  ),\n",
    "    'fwdPrice'        : ( 0.50, 1.50 ),\n",
    "    'initial_vol'     : ( 0.10, 0.50 ),\n",
    "    'long_vol_ratio'  : ( 0.50, 2.00 ),\n",
    "    'decay_half_life' : ( 1.00, 5.00 ),\n",
    "    'rho'             : (-0.80, 0.80 ),\n",
    "    'feller_factor'   : ( 0.01, 4.00 ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some representative volatility smiles in Heston model are illustrated in the graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from heston_volatility_plots import plot_smiles\n",
    "from heston_volatility_plots import get_widgets\n",
    "interact(\n",
    "    plot_smiles,\n",
    "    **get_widgets([1/12, 0.5, 1.0, 2.0], label_dict)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Chebyshev Interpolation\n",
    "\n",
    "In this section we setup Chebyshev interpolations using Numpy, Tensorflow and Julia. First we setup the interpolations and check for consistency between the implementations. This step also illustrates the usage of each of the method. Then we test and compare the computational performance of the methods.\n",
    "\n",
    "We need a few packages and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')  # make sure we find our src/ folder\n",
    "\n",
    "import numpy as np\n",
    "import QuantLib as ql\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target function $f$ is implemented using QuantLib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heston_model_pricer import HestonModelPricer\n",
    "from heston_model_pricer import vector_to_params\n",
    "\n",
    "pricer = HestonModelPricer(integration=ql.AnalyticHestonEngine_Integration.gaussLaguerre(192))\n",
    "f = lambda x : pricer.implied_volatility(*vector_to_params(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower and upper boundaries for interpolation are also setup consistently across approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([label_dict[k][0] for k in label_dict])\n",
    "b = np.array([label_dict[k][1] for k in label_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical configuration of Chebyshev interpolation is the **polynomial degree per dimension** $\\left(N_d\\right)_{d=1,\\ldots,D}$. This configuration parameter controls interpolation accuracy and computational effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [\n",
    "    2,  # term\n",
    "    2,  # moneyness\n",
    "    2,  # fwdPrice\n",
    "    2,  # initial_vol\n",
    "    2,  # long_vol_ratio\n",
    "    2,  # decay_half_life\n",
    "    2,  # rho\n",
    "    2,  # feller_factor\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of Chebyshev interpolation points becomes $\\prod_{d=1}^D \\left(N_d + 1\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = np.prod([ Nd+1 for Nd in degrees ])\n",
    "n_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For interpolation testing we use a set of random point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "base2 = 13\n",
    "X_test = a + np.random.uniform(size=(2**base2, 8)) * (b-a)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np.apply_along_axis(f, 1, X_test)\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.multivariate_chebyshev_numpy import chebyshev_multi_points  as chebyshev_multi_points_np\n",
    "from src.multivariate_chebyshev_numpy import chebyshev_transform     as chebyshev_transform_np\n",
    "from src.multivariate_chebyshev_numpy import chebyshev_coefficients  as chebyshev_coefficients_np\n",
    "from src.multivariate_chebyshev_numpy import chebyshev_interpolation as chebyshev_interpolation_np\n",
    "\n",
    "Z_np = chebyshev_multi_points_np(degrees)\n",
    "X_np = chebyshev_transform_np(Z_np, a, b)\n",
    "X_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X_{\\text{np}}$ represent the multivariate Chebyshev points on the target function domain for which we need to calculate function values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values  = np.apply_along_axis(f, 1, X_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chebyshev coefficient tensor calculation represents the calibration of the interpolation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_np = chebyshev_coefficients_np(degrees, Z_np, values)\n",
    "C_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the interpolation on our set of random test points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_np = chebyshev_interpolation_np(X_test, C_np, a, b)\n",
    "Y_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_np - Y_test, bins=20)\n",
    "plt.xlabel('absolute deviation (Numpy)')\n",
    "plt.ylabel('number of test samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tf_config import tensorflow as tf\n",
    "    # control tf version via tf_config if necessary\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "from src.multivariate_chebyshev_tensorflow import chebyshev_multi_points  as chebyshev_multi_points_tf\n",
    "from src.multivariate_chebyshev_tensorflow import chebyshev_transform     as chebyshev_transform_tf\n",
    "from src.multivariate_chebyshev_tensorflow import chebyshev_coefficients  as chebyshev_coefficients_tf\n",
    "from src.multivariate_chebyshev_tensorflow import chebyshev_interpolation as chebyshev_interpolation_tf\n",
    "\n",
    "Z_tf = chebyshev_multi_points_tf(degrees)\n",
    "X_tf = chebyshev_transform_tf(Z_tf, a, b)\n",
    "X_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow and Numpy implementation use the same ordering of Chebyshev points. Consequently, we can re-use the target function values for Chebyshev coefficient tensor calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_tf = chebyshev_coefficients_tf(degrees, Z_tf, values)\n",
    "C_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can again interpolate the function at our random test points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tf = chebyshev_interpolation_np(X_test, C_tf, a, b)\n",
    "Y_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_tf - Y_test, bins=20)\n",
    "plt.xlabel('absolute deviation (Tensorflow)')\n",
    "plt.ylabel('number of test samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia Implementation\n",
    "\n",
    "Julia's Chebyshev implementation is incorporated via PyJulia package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from julia.api import Julia\n",
    "jl_instance = Julia(compiled_modules=False)  # avoid Julia and PyJulia setup error.\n",
    "from julia import Main as jl\n",
    "jl.include('../../src/multivariate_chebyshev_julia.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_jl = jl.chebyshev_multi_points(degrees)\n",
    "X_jl = jl.chebyshev_transform(Z_jl, a.reshape((1,-1)), b.reshape((1,-1)))\n",
    "X_jl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Julia we calculate the same Chebyshev points. But the cartesian product implementation uses a different ordering along the dimensions. Consequently, we need to re-calculate the function values for that the ordering in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_jl = np.apply_along_axis(f, 1, X_jl)  # different ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting Chebyshev coefficient tensor is equivalent to the Numpy or Tensorflow implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_jl = jl.chebyshev_coefficients(degrees, Z_jl, values_jl, jl.matmul)\n",
    "C_jl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can interpolate function values at our random test points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_jl = jl.chebyshev_interpolation(X_test, C_jl, a.reshape((1,-1)), b.reshape((1,-1)), jl.matmul)\n",
    "Y_jl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_jl - Y_test, bins=20)\n",
    "plt.xlabel('absolute deviation (Julia)')\n",
    "plt.ylabel('number of test samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Implementations\n",
    "\n",
    "We verify that Numpy, Tensorflow and Julia implementation yield equivalent results.\n",
    "\n",
    "First we check the Chebyshev coefficient tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tensorflow versus Numpy: %.2e' % np.max(np.abs(C_tf - C_np)))\n",
    "print('Julia versus Numpy:      %.2e' % np.max(np.abs(C_jl - C_np)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Tensorflow uses single precision (Float32) floating point numbers as default. Numpy and Julia use double precision floating point numbers. This is reflected in the numerical differences observed above.\n",
    "\n",
    "Similarly, we compare the interpolated function values for the random test points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tensorflow versus Numpy: %.2e' % np.max(np.abs(Y_tf - Y_np)))\n",
    "print('Julia versus Numpy:      %.2e' % np.max(np.abs(Y_jl - Y_np)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from numerical differences due to rounding errors we find that all three implementations yield consistent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis\n",
    "\n",
    "We analyse the computational performance of the methods. For this exercise we use the eight-dimensional target function $f$ for modelling Heston implied volatilities.\n",
    "\n",
    "We use an equal number of degrees $N=N_d$ across all dimensions $d=0,\\ldots,7$ and let $N$ run from $1$ to $3$.\n",
    "\n",
    "As benchmark we consider the Chebyshev coefficient calculation (*chebyshev_coefficients*). This routine has computational effort proportional to the square of number of Chebyshev points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heston_chebyshev_performance import run_performance_testing\n",
    "\n",
    "root_path = '../../'  # assume we start script from examples/.\n",
    "perf_degrees = [ 1, 1, 1, 2, 2 ]  # N=3 runs for about 10 mins.\n",
    "res = run_performance_testing(perf_degrees, label_dict, pricer, root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(res)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af98fb13c0682ca4fe3350401d42f2a404b34a8b53a566210d6d775d501366cd"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
